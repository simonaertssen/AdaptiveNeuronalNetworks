% !TEX root = ../main.tex
\newpage
\section{\mywork Mean Field Reductions for Undirected Graphs} \label{sec:MFRSUndirected}
We will now investigate the questions that were raised after deriving the \MFR. How do we deal with the curse of dimensionality concerning the degree distribution? If the synchronisation dynamics of the network of Theta neurons in \eqref{eq:thetaneuronnetwork} can be predicted by the Ott-Antonsen reductions \eqref{eq:OttAntonsenMeanField}, then it can also be measured by the order parameter \eqref{eq:orderparameter}. These systems describe the same quantity, but how can we show that?


\subsection{Directed graphs as permutations}
%So how can we use the \MFR efficiently when the network is a directed graph with an asymmetrical adjacency matrix? Let us investigate.
So how can we use the \MFR when the network is a directed graph?
\begin{list}{$\bullet$}{}  
\item Sampling $\kinb$ and $\koutb$ from a bivariate distribution requires us to find the marginal distribution of $P$ for $\kinb$, sampling $\kinbi$, and then sampling $\koutbj$ from $P$ while keeping $\kinbi$ fixed. This is a cumbersome process. And what relation would there be between $\kinb$ and $\koutb$?
\item However, if we assume that the marginal distributions for $\kinb$ an $\koutb$ are independent, there is a simplification to be found. We can even assume that the two marginal distributions are identical univariate distributions. 
\item Hence, we can sample $\kinb$ from a univariate distribution and find $\koutb = \permute ( \kinb )$ so that the total number of links remains constant. This is an important trait, as we do not have to rely on the sheer size of the network to yield a constant number of links on average.
\end{list}

This hypothesis can be tested: we assume that $P(\k) = P(\kin) \cdot P(\kout)$ so that $P$ consists of two identical and independent distributions, given by the distributions presented in Chapter \ref{sec:NetworkTopologies}. Then, we sample $\kinb \sim P(\kin)$ and perform a permutation to find $\koutb$. The surface given by $P(\k)$ and the histogram of $\k_j$ have been plotted in Figure \ref{fig:2Ddistributions}. As we can see, the variates follow the distribution well. 

\begin{figure}[ht]
\centering
\includegraphics[width = \textwidth]{../Figures/Distributions/2D.pdf}
\caption{Bivariate distributions for different network topologies, using 10$^4$ number of samples. The surface given by $P(\k)$ is well approximated by the histogram of variates sampled from a univariate distribution, used as the marginal distribution. $\kmean =  2 \times 10^3$ for all topologies, $p \approx 0.2$ for the random network and $\gamma = 4.3$ for the scale-free network.}
\label{fig:2Ddistributions}
\end{figure}
%Hence, we can use univariate distributions in our simulations of the Ott-Antonsen Mean Field \eqref{eq:OttAntonsenMeanField}.
However, the problem remains the same: $\K$ is too large to simulate the dynamics of the network. 

What we can do, is use $P(k)$ in the Ott-Antonsen reduction for a symmetric network, and observe how much the solution of the asymmetric network differs from the reduction of the symmetric network. This is an attainable goal.

\subsection{Building the adjacency matrix} \label{sec:buildingA}
If we want to simulate the network of theta neurons we need to construct the adjacency matrix. We can find an exact solution for $A$ given the degree vectors in \eqref{eq:definekinkoutfromP}. $A_{ij}$ represents a directed graph, but $A_{ij} \neq A_{ji}$ is not a necessary condition. For the elements of $A_{ij}$ we need to find $N^2$ number of variables. We have the following constraints:
\begin{enumerate}
\item The column- and row-sums of $A_{ij}$ must be equal to $\kinb$ and $\koutb$, see (\ref{eq:definekinkoutfromA}). 2$N$ constraints.
\item Self-coupling is mandatory: $A_{ii} = \boldsymbol{1}$. $N$ constraints. \cite{OttAntonsen2017}
\item The total number of links is constant: $\sum_{i=1}^{N} \kinbi \equiv \sum_{j=1}^{N} \koutbj \equiv \sum_{i,j=1}^{N}A_{i j}$. 1 constraint.
\end{enumerate}
This means that there are $N^2 - (3N + 1)$ variables to find. Once a solution has been found, $A_{ij}$ can be switched with element $A_{ic}$ if $A_{ij} \neq A_{ic}$ and $A_{rj}$ with $A_{rc}$, which yields another feasible solution. The solutions to this problem are thus bound by permutation symmetry. The number of switches one can make is high, and therefore we can simply try a stochastic approach to obtain $A$:
\begin{enumerate}
\item Choose a random row $i \in [1,N]$. $A_{i,i} = 1$, so we need $m = \kinbi - 1$ elements that are 1.
\item Perform $\permute ( \koutbj, j \neq i)$ and therein find the indices $\boldsymbol{\ell}$ of the $m$ first largest elements. 
\item Set $A_{il} = 1 \: \: \forall \: \: l \in \permuteinv (\boldsymbol{\ell})$.
\end{enumerate}
Algorithms that find the largest value in a vector start from the first or the last element. The permutation in step 2 allows us to find different maxima every time by shuffling the row, which greatly reduces the event that constraint 1 does not hold. In practice, this stochastic method finds a solution for $A$ within 5 tries.

\begin{figure}[H]
\centering
\includegraphics[width = \textwidth]{../Figures/Adjacency_matrices.pdf}
   \caption{Adjacency matrices for different types of networks with $N$ = 500 and $\kmean$ = 100. We can see how the fixed-degree network is quite homogeneous, while the random network shows some more clustering. The scale-free network has a low number of nodes with a very high degree, which is why we see vertical and horizontal stripes in the adjacency matrix.}
   \label{fig:adjacencymatrices}
\end{figure}



\subsection{Initial conditions: analytical versus numerical approaches} \label{sec:initialconditions}
As our goal is to compare theory and simulations, we need to be able to start both at the exact same condition. This notion requires us to transform between the three number sets that our dynamics are described in: $\theta \in \T^N$, $z \in \C^{M_{\k}}$ and $Z, \bar{Z} \in \C$. It is really only necessary to find a transformation that holds accurately for $t=0$, as the distribution of $\theta$ and $z$ over their number set is unknown, but we assume they converge to that distribution when the systems are computed. 

%As the systems in \cref{eq:orderparameter,eq:OttAntonsenMeanField,eq:MeanField} describe the same dynamics for fully connected networks, it is important to be able to transform initial conditions between systems. If we expect their behaviour to be the same, then we need to test that by starting from the same point in time. Hence we can test whether \textsl{macroscopically} we can find the same equilibria, but we can also test \textsl{microscopically} whether the systems arrive at those points at the same time. 

%The order parameter \ref{eq:orderparameter} and the mean-field order parameter \eqref{eq:OttAntonsenMeanField} describe the same dynamics for fully connected networks. Testing this hypothesis 

%  If the initial conditions of all systems are exactly the same, then we should find that they describe the exact same behaviour.
% - macroscopically and microscopically - and this is the best way to test the \MFR theory. 

%When transforming between $\theta_i(t) \leftrightarrow z(\k,t) \leftrightarrow Z(t)$ we go from $\T^N \leftrightarrow \C^{M_\k} \leftrightarrow \C$. If we have the same initial conditions, then all systems will predict the same behaviour. We will only map everything to $\C$.\\
As we can optimally study the behaviour of $Z$ and $\bar{Z}$ in the complex unit circle, the most important transformations are those that yield $\theta$ and $z$ from $Z$ and $\bar{Z}$ respectively. Hence, we can start our simulations anywhere in $\C$, close to the limit cycle in Figure \ref{fig:MFRCPW} for example. Our analysis will benefit from this advantage. \\

%As we can study the behaviour of $Z$ and $\bar{Z}$ in the complex unit circle, the most important relation we need to find is the transformation from $\C$ to $\T^{N}$ and from $\C$ to $\C^{M_{\k}}$, which yield the phase angles $\theta_i(0)$ and the degree dynamics $z(\k,0)$ from $Z(0)$ and $\bar{Z}(0)$ respectively. \\

% The following maps can be used to transform the initial conditions, but as they do not give any qualitative information on the dynamics or distributions of the variables, they are not valid for transforming between dynamics. \\

Let us start with the simplest transformation. Given an initial phase angle $\theta_i(0)$ or initial degree dynamics $z(\k, 0)$ we wish to find their resulting description in the complex unit circle. Mapping operations onto the order parameter is straightforward using \eqref{eq:orderparameter} and \eqref{eq:OttAntonsenMeanField}:
\begin{align}
\theta_i(0) \xrightarrow{\hspace*{8mm}} Z(0) &= \frac{1}{N} \sum_{j=1}^N e^{\ic\theta_j(0)} \label{eq:thetatoZ}\\
z(\k,0) \longrightarrow \bar{Z}(0) &= \frac{1}{N} \sum_{\k \in \K} P(\k) z(\k, 0)  \label{eq:ztoZ}
\end{align}
Here we can immediately see that information about the distribution of $\theta$ and $z$ is lost when taking the (weighed) average. 

Starting from an initial synchronization $Z(0)$ and taking the inverse tranfsormation, we can make use of the fact that a set of identical values has an average equal to that value. This is simple for $\theta_i(0)$: we can take all phase angles to be the same at $t = 0$. For $z(\k,0)$ we have a weighed average which we need to invert, while making sure that the whole sums up to $N$ by multiplying with the total number of neurons $n(\k)$ of degree $\k$:
\begin{align}
Z(0) \xrightarrow{\hspace*{8mm}} \theta_i(0) &= -\ic \cdot \log \left( Z(0) \right)  \label{eq:Ztotheta} \\
Z(0) \longrightarrow z(\k,0) &= \frac{Z(0) \cdot n(\k)}{P(\k)} \label{eq:Ztoz}
\end{align}
It is necessary to include $n(\k)$, as $P$ is only accurate in the limit that $N \rightarrow \infty$. This approach only alters the magnitude of $Z(0)$, so that $z(\k,0)$ will be distributed on a line through $Z(0)$. Then, transforming between $\theta_i$ and $z(\k)$, we need to filter $\theta_i$ per degree as there exist $n(\k)$ number of nodes with $\degree ( \theta_i ) = \k$:
\begin{alignat}{2}
z(\k,0) \longrightarrow \theta_i(0) &= -\ic \cdot \log \left( \frac{z(\k)\cdot P(\k)}{n(\k)} \right) \qquad &&\forall \: \theta \in \{ \theta \: | \: \degree(\theta) = \k \}  \label{eq:ztottheta}\\
\theta_i(0) \longrightarrow z(\k,0) &= \sum_{\k} e^{\ic \vartheta_{\k}} \qquad \qquad &&\forall \: \vartheta_{\k} \in \{ \vartheta_{\k} = \sum^{n(\k)} \theta \: | \: \degree(\theta) = \k \} \label{eq:thetatoz}
\end{alignat}
%We can see how $\lim_{N \rightarrow +\infty} n(\k) = P(\k)$, which makes these maps exact for any network size. 
%The reason that these transformations can only hold for the initial state is because it is currently unknown what distributions $\theta_i(t)$ and $z(\k,t)$ should have. That information is lost when taking the (weighed) average in \eqref{eq:orderparameter} and \eqref{eq:MeanField}. \\

The relations derived here raise problems when $P(\k)$ spans different orders of magnitude. \eqref{eq:Ztoz} does not bound $z$ to its set, so it might occur that the distribution of $z$ has values outside of the complex unit circle. However, transforming back to $\bar{Z}$ will always be correct. This problem does not occur for $\theta$, as $\T$ is a one-parameter group so that multiplication and division of elements in the group remain in the group. Let us look at the example in Figure \ref{fig:mappings}, where we are trying to find $z(\k, 0)$ so that $\bar{Z}(0)$ is equal to the desired initial condition $Z(0) = $ -0.2 + $\ic$0.8, using a scale-free topology. \\

%The initial values of $z$ are not bound on $\C$, so it might occur that the initial condition given through \eqref{eq:ztoZ} is exact, but that the distribution of $z(\k,t)$ leaves us with some out of the complex unit circle. Let us look at the example in Figure \ref{fig:mappings}, where we are trying to find $z(\k, 0)$ so that $\bar{Z}(0)$ is equal to our desired initial condition $Z(0) = $ -0.2 + $\ic$0.8. \\

% Especially for scale-free networks, where there is a large difference between the smallest and largest degree, this offset is large
 
When simply taking all $z(\k, 0) = Z(0)$, there is a slight offset between $Z(0)$ and $\bar{Z}(0)$. However, the dynamics are well-behaved and the end-state is almost a smooth curve. One can really interpret this curve as the attractive manifold of the Ott-Antonsen reduction. This method is an easy way of quickly coming up with an initial condition, without requiring any computation. In general, this yields quite a good approximation. \\

\begin{figure}[H]
\centering
\includegraphics[width = \textwidth]{../Figures/PhaseSpace/Mappings.pdf}
   \caption{Simulation of 1000 neurons in a scale-free network. Example on the importance of accurate initial conditions. A scalefree network is used to show the outcome of different strategies of initialising the network. Left: the initial condition is not correct, but it yields very smooth dynamics. Middle: the initial condition is correct, but it does not yield smooth dynamics. Right: the initial condition is correct, and it also yields smooth dynamics.}
   \label{fig:mappings}
\end{figure}

When using \eqref{eq:ztoZ}, we can see that the initial conditions lie on a straight line through the origin indeed, and that $\bar{Z}(0)$ is exactly equal to $Z(0)$. When a given $\k$ yields a small $P(\k)$, $z(\k,0)$ will be scaled away from the origin. This means that the dynamics of nodes of that degree are not well represented. However, their contribution to $\bar{Z}$ in \eqref{eq:OttAntonsenMeanField} is small, so sometimes these effects cancel out and the dynamics are in fact quite smooth. However, we can see that in our example the dynamics are not represented well, resulting in large errors after conception and a more random end-state. We do expect these effects to cancel out after longer periods and for larger $N$, as the manifold is attractive and larger networks cancel out outliers, but our aim is to be as precise as possible from $t=0$. \\

When trying to address the problems that are encountered here, we can try and find the distribution of $z(\k,0)$ numerically. We can solve for the root of $f(z) = \| Z(0) - \bar{Z}(0) \|$ (where $\bar{Z}$ is computed from $z$) under the constraint that $| z | \leq 1$, starting from an initial guess $\hat{z}$ clustered around $Z(0)$. The resulting initial distribution is also quite clustered but as it is mostly a result of the constraint, we are more interested in the end-state, which shows a lot of improvements over the second method. The initial conditions are exact (up to $10^{-6}$) and the dynamics are smooth, which makes this method the most desirable. However, convergence can be very slow for scale-free networks, and the complexity of the system to solve scales with $\sim N^2$ . It is therefore necessary to judge which of the three methods to use when performing a new simulation.


\subsection{Final conditions}
Given that the final condition of the system is such a particular smooth curve, we can try and understand what kind of distribution $z$ follows on that curve. In Figure \ref{fig:FinalConditions} we have made different networks converge to a stable node in the \PSR state until changes to the system were smaller than a tolerance. We can see that the final condition of $z$ is close to the final condition of $\bar{Z}$. If we divide the length of the curve into equal parts, we can count how many $z$ can be found in each interval. The resulting distributions difficult to interpret, but we can see some traits of their respective degree distributions, though the likeness is not very high. 
\begin{figure}[H]
\centering
\includegraphics[width = \textwidth]{../Figures/Distributions/FinalConditions.pdf}
\caption{Final conditions of $z$ for the \PSR state, represented by 40 equally spaced indices and the resulting histogram. Both the random and scale-free network topologies show that the final conditions of $z$ are a smooth curve, and that points on that curve have a particular distribution over the length of the curve.}
\label{fig:FinalConditions}
\end{figure}

For other macroscopic states the distributions of $z$ across its final condition are quite similar as presented here. However, when the scale-free network has converged to the stable focus in the \PSS state, the final condition is a highly convoluted spiral, which is difficult to interpret. What is important though, is that there is a definite structure to be found in the final condition as well.


\subsection{Commutativity of complex vectors} 
It is important to notice that in \eqref{eq:OttAntonsenSystemFull} and \eqref{eq:OttAntonsenMeanField} and many other equations in this work, we compute an inner vector product, which is non-commutative for complex numbers:
\begin{align}
a \cdot b = (b \cdot a)^c \qquad a, b \in \c^r
\end{align}
This is the result of the \textsl{Conjugate} or \textsl{Hermitian} symmetry of the inner product. This is especially important in the implementation, as one needs to be consistent with left- or right-hand products.


\subsection{Fixpoint iteration}
In \cite{OttAntonsen2017} a fixpoint iteration is suggested to find attractive fixpoints of the system \eqref{eq:OttAntonsenSystemFull}. If we set $\frac{\partial z(\k, t)}{\partial t} = 0$ we can solve the following system:
\begin{align}
\ic \frac{(z(\k, t)-1)^{2}}{2} &= \frac{(z(\k, t)+1)^{2}}{2} \cdot I(\k, t) \nonumber \\
\ic \left(\frac{z(\k, t)-1}{z(\k, t)+1}\right)^2 &= I(\k, t) \nonumber \\
\frac{z(\k, t)-1}{z(\k, t)+1} &\equiv b(\k,t) \nonumber \\
z(\k, t) - 1 &= b(\k,t) z(\k, t) + b(\k,t)  \nonumber \\
z(\k, t) \cdot (1 - b(\k,t)) &= b(\k,t)  + 1\nonumber
\end{align}
We can then obtain the stable equilibria from:
\begin{align}
\ic b(\k,t)^2 = I(\k, t) \hspace{10mm} z(\k, t)_{\pm} = \frac{1 \pm b(\k,t)}{1 \mp b(\k,t)} \label{eq:fixedpointiterations} 
\end{align}
where the signs are chosen so that $\vert z(\k, t) \vert \leq 1$. This works well, and in general this method converges fast.


\subsection{A Newton-Raphson iteration for all fixpoints}
The fixpoint iteration \eqref{eq:fixedpointiterations} only gives us the stable equilibria of the \MFR. We can obtain all equilibria and their stability through the Jacobian from a Newton-Raphson iteration, which has been described in \ref{app:NewtonRaphson}. The Jacobian would be a $M_{\k} \times M_{\k}$ matrix, as we have $M_{\k}$ unique degrees in the network, and we need to take the derivate of one with respect to each other. However, finding the Jacobian is a challenge, as \eqref{eq:OttAntonsenSystemFull} is non-holomorphic: $H_2(\k,t)$ does not satisfy the Cauchy-Riemann equations. We can show this by separating $z$ into its real and imaginary part and expressing $H_2(\k,t)$ as two real-valued functions $u$ and $v$:
%For \eqref{eq:OttAntonsenSystemFull}, we can compute the Jacobian for the diagonal and off-diagonal elements separately. But as $z(\k,t)$ is a complex function, first we need to understand what the derivative of a complex function is. 
\begin{align*}
z(\k,t) &= x(\k,t) + \ic y(\k,t) \qquad \qquad x, y \in \R^{M_{\k}}\\
f\left( z(\k,t) \right) &= u\left(x(\k, t), y(\k, t) \right) + \ic v\left(x(\k, t), y(\k, t) \right)\\
&= \frac{\kappa}{\kmean} \sum_{\kacc} P\left(\kacc\right) \: a\left(\kacc \rightarrow \k\right) \left( 1 + \frac{z(\kacc, t)^2 + (z(\kacc, t)^c )^2}{6} - \frac{4}{3} \Re(z(\kacc, t)) \right)\\
&= \frac{\kappa}{\kmean} \sum_{\kacc} P\left(\kacc\right) \: a\left(\kacc \rightarrow \k\right) \left( 1 + \frac{x(\kacc, t)^2}{3} - \frac{4}{3} x(\kacc, t) \right)
\end{align*}
This leaves us with only $u$ defined as a real-valued function, so that the Cauchy-Riemann equations do not hold as $v$ is zero. Thus we cannot express the Jacobian as a matrix of complex numbers. \\

Instead, we must think of $z$ as a vector of real and imaginary parts and express it as $z(\k,t) = [ x(\k,t), \: y(\k,t)]$. We can then interweave the two parts in the Jacobian, forming a $2 M_{\k} \times 2 M_{\k}$ matrix of real values. For fixed-degree networks this is easy, and the approach yields the well-known $2 \times 2 $ Jacobian, which has been used in Figure \ref{fig:macroscopicstatesfixeddegree} to signify the stability of the equilibrium points and the magnitude and direction of the eigenvalues. \\

However, for the typologies with more than one unique degree in the network, this approach did not yield promising results. If convergence occurs, the the equilibrium is found outside of the unit circle, which is unphysical. One issue might be that the initial condition used for the Newton-Raphson iteration is still not close enough to the true manifold. The resulting equilibria might be due to residuals from the Fourier series applied to $f(\theta, \eta | \k, t)$ in \eqref{eq:meanfieldorderparameter}. A deeper understanding of the manifold and initial conditions is necessary. \\

Otherwise either the derivation of the Jacobian or the execution of the algorithm is flawed, though a mistake could not be found. For now, we will revert to the fixed point iteration method, \eqref{eq:fixedpointiterations}, to find stable equilibria of the system.

%\textcolor{red}{QUESTION}: \textsl{explain how the Jacobian can be found as a $2 M_{\k} \times 2 M_{\k}$ matrix by using $z(\k,t) = [ x(\k,t), \: y(\k,t)]$ and interweaving the $x$ and $y$ dimension in the matrix, see \cite{Cross2018}. This has been implemented but only stable results for the \PSR state. Right now I still can only find attractive fixpoints. Should I continue on this?}


\subsection{Fixed-degree networks as a baseline}
Now we have all the necessary tools to simulate networks of theta neurons: the adjacency matrix, and an understanding of the initial conditions. First, we will use a fixed-degree network, as this is the most simple instance of the different topologies. As all nodes have the same degree, the dynamics of a symmetric and asymmetric fixed-degree network ought to be identical. The results are shown in Figure \ref{fig:InspectMeanFieldFixedDegree}.

We can easily recognise the three macroscopic states from Figure \ref{fig:macroscopicstatesfixeddegree}. It is no wonder that the solutions for the equilibria are accurate, though results for the limit cycle are quite remarkable. There are small differences between $Z$ measured by the whole network and $\bar{Z}$ predicted by the \MFR, but these are most likely due to a finite network size and a finite integration step. The mean-field systems \eqref{eq:OttAntonsenMeanField} and \eqref{eq:MeanField} yield the exact same behaviour, as expected. This test benchmarks the lowest amount of error we can observe between simulation and theory, as for fixed-degree networks (\ref{eq:OttAntonsenSystemFull}) consists of a single equation.
\begin{figure}[H]
\centering
\includegraphics[width = \textwidth, trim={0 3mm 0 3mm},clip]{../Figures/InspectMeanFieldFixedDegree.pdf}
\caption{Comparison of the simulation of a fixed-degree network of Theta neurons and the Ott-Antonsen theory by the magnitude of the order parameter. We observe that the same three macroscopic states are found by the three descriptions of the mean-field.}
\label{fig:InspectMeanFieldFixedDegree}
\end{figure}


\subsection{Results for arbitrary network topologies} \label{sec:resArbNetw}
The dynamics of random networks seem to be very similar to fixed-degree networks, when looking at the dynamics in the unit circle. Both networks have a unimodal and symmetric degree distribution, and this might be the cause for the likeness. We can see in Figure \ref{fig:MFOARCPW_random} that the limit cycle is slightly larger, but no other differences can be observed. When looking at the dynamics over time the results in Figure \ref{fig:InspectMeanFieldRandom} are also consistent, with a little more deviation between simulation and theory in the \CPW state, but this can be expected for finite networks.
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.32\linewidth}
   \centering
  \includegraphics[width=\linewidth]{../Figures/PhaseSpace/MFOARPSR_random.pdf}
   \caption{PSR state for $\eta_0 = -0.9, \sigma = 0.8$ and $\kappa= -2$. The mean field settles onto a stable node.}
   \label{fig:MFOARPSR_random} 
\end{subfigure} \hfill
\begin{subfigure}[b]{0.32\linewidth}
   \centering
  \includegraphics[width=\linewidth]{../Figures/PhaseSpace/MFOARPSS_random.pdf}
   \caption{PSS state for $\eta_0 = 0.5, \sigma = 0.7$ and $\kappa= 2$. The mean field settles onto a stable focus.}
   \label{fig:MFOARPSS_random}
\end{subfigure} \hfill
\begin{subfigure}[b]{0.32\linewidth}
   \centering
  \includegraphics[width=\linewidth]{../Figures/PhaseSpace/MFOARCPW_random.pdf}
   \caption{CPW state for $\eta_0 = 10.75, \sigma = 0.5$ and $\kappa= -9$. The mean field settles onto a stable limit cycle.}
   \label{fig:MFOARCPW_random}
\end{subfigure}
   \caption{Three macroscopic states observed in the \MFR using a random network, inside the imaginary unit circle $|Z(t)| \leqslant 1$. Green arrows mark the phase space vector field and green trails mark solution curves. The dotted line in the \CPW state is the limit cycle of the fixed-degree networks, added for reference.}
   \label{fig:macroscopicstatesrandomnetworks}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width = \textwidth, trim={0 3mm 0 3mm},clip]{../Figures/InspectMeanFieldRandom.pdf}
\caption{Comparison of the simulation of a random network of Theta neurons and the Ott-Antonsen theory by the magnitude of the order parameter. }
\label{fig:InspectMeanFieldRandom}
\end{figure}

For scale-free networks, we can see that again the three macroscopic states continue to exist, Figure \ref{fig:macroscopicstatesscalefreenetworks}. However, it seems like there is a fairly large discrepancy between symmetric and asymmetric networks, in Figure \ref{fig:InspectMeanFieldScaleFree}. The stable node in the \PSR state is found at different locations, and the limit cycle in the \CPW state seems to be very different, but with a similar period. Indeed, if we look at the limit cycle to which the dynamics are attracted to in Figure \ref{fig:InspectMeanFieldScaleFreePhaseSpace} we can indeed see two distinct cycles. \\

This means that due to its topology, the scale-free network cannot be represented by a symmetric variant. The degree distribution is asymmetric, and this likely causes the asymmetry observed here. Another observation is that now the fixed-point iteration finds the centre of the limit cycle as a stable equilibrium. The limit cycle has always been observed to be attractive, so a stable equilibrium within would require another unstable limit cycle around the equilibrium. We will regard this as an error.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.32\linewidth}
   \centering
  \includegraphics[width=\linewidth]{../Figures/PhaseSpace/MFOARPSR_scalefree.pdf}
   \caption{PSR state for $\eta_0 = -0.9, \sigma = 0.8$ and $\kappa= -2$. The mean field settles onto a stable node.}
   \label{fig:MFOARPSR_scalefree} 
\end{subfigure} \hfill
\begin{subfigure}[b]{0.32\linewidth}
   \centering
  \includegraphics[width=\linewidth]{../Figures/PhaseSpace/MFOARPSS_scalefree.pdf}
   \caption{PSS state for $\eta_0 = 0.5, \sigma = 0.7$ and $\kappa= 2$. The mean field settles onto a stable focus.}
   \label{fig:MFOARPSS_scalefree}
\end{subfigure} \hfill
\begin{subfigure}[b]{0.32\linewidth}
   \centering
  \includegraphics[width=\linewidth]{../Figures/PhaseSpace/MFOARCPW_scalefree.pdf}
   \caption{CPW state for $\eta_0 = 10.75, \sigma = 0.5$ and $\kappa= -9$. The mean field settles onto a stable limit cycle.}
   \label{fig:MFOARCPW_scalefree}
\end{subfigure}
   \caption{Three macroscopic states observed in the \MFR inside the imaginary unit circle $|Z(t)| \leqslant 1$. Green arrows mark the phase space vector field and blue trails mark solution curves. Red points indicate equilibrium points, found by the fixed-point iteration. The dotted line in the \CPW state is the limit cycle of the fixed-degree networks, added for reference}
   \label{fig:macroscopicstatesscalefreenetworks}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width = \textwidth, trim={0 3mm 0 3mm},clip]{../Figures/InspectMeanFieldScaleFree.pdf}
\caption{Comparison of the simulation of a scale-free network of Theta neurons and the Ott-Antonsen theory by the magnitude of the order parameter.}
\label{fig:InspectMeanFieldScaleFree}
\end{figure}

The results of these experiments point us in the right direction. Symmetric and asymmetric networks of the same family share many macroscopic properties, and networks with a symmetric degree distribution might share these properties with more detail. 

\begin{figure}[H]
\centering
\includegraphics[width = 0.5\textwidth]{../Figures/PhaseSpace/ScalefreeLimCycles.pdf}
\caption{Comparison of the limit cycles found by theory and simulation.}
\label{fig:InspectMeanFieldScaleFreePhaseSpace}
\end{figure}


